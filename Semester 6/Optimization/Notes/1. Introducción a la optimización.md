En este apartado se tratan las temáticas introductorias a la optimización
## Línea de vida de un proceso de optimización

Los procesos de optimización, suelen llevar una línea de vida, está línea podría variar dependiendo del objetivo de implementar optimizaciones y el criterio de quien lo implemente, sin embargo, la línea de vida que implementaremos en este contexto será la siguiente.

1. **Formulación del problema:** Identificación y definición precisa del problema.
2. **Construcción del modelo:** Desarrollo de un modelo matemático.
3. **Solución del modelo:** Aplicación de métodos y algoritmos para encontrar soluciones.
4. **Validación del modelo:** Verificación del modelo y soluciones.
5. **Implementación:** Aplicación de soluciones en el entorno real.
## Conocimientos previos

Para que una función $y$ de **una variable** $x$ sea optimizable, es necesario cumplir dos propiedades fundamentales.

* Debe ser continua en el intervalo $[a,b]$.
* Debe ser diferenciable **por lo menos** 2 veces.

Recordemos que la primera derivada de la función objetivo determina los puntos críticos de la misma, mientras que la segunda derivada determina convexidad-concavidad. Para ello es necesario realizar el siguiente proceso base de la optimización

1. Se encuentran los puntos críticos $(x=c)$ con la primera derivada de la función.
2. Se realiza la prueba de la **segunda derivada**, donde se encuentrar los siguientes casos.

	* Sí $f''(c) < 0$, entonces $x=c$ es un máximo local.
	* Sí $f''(c) > 0$, entonces $x=c$ es un mínimo local.
	* Sí $f''(c) = 0$, entonces $x=c$ no hay extremos *(max o min)*.

En funciones de varias variables *(para simplificar consideremos únicamente dos variables)*, se poseen los siguientes teoremas.

> [!Info] **Teorema 1**
> 
> Si $f(x,y)$ tiene un máximo o mínimo local en $(x_0, y_0)$, entonces:
> $$f_x(x_0, y_0) = f_y(x_0, y_0) = 0$$

Este teorema nos explica la existencia de puntos críticos $(x_0, y_0)$ en la primera derivada respecto a cada variable.
 
> [!Info] **Teorema 2**
> 
> Sea $(x_0, y_0)$ un punto crítico de la **función objetivo** $f_x(x_0, y_0) = f_y(x_0, y_0) = 0$ 
> 
> Y sea $\Delta(x,y) = f_{xx}(x,y)f_{yy}(x,y) - \left[f_{xy}(x,y)\right]^{2}$
> 
> Se entiende como máximo y mínimo 
> 
> | $f_{xx}(x_0, y_0)$ | $f_{yy}(x_0, y_0)$ | $\Delta(x_0, y_0)$ | Optimiza |
> | :--: | :--: | :--: | :--: | 
> | $<0$ | $>0$ | $>0$ | máximo local en $(x_0, y_0)$ |
> | $>0$ | $>0$ | $>0$ | mínimo local en $(x_0, y_0)$ |

Este teoriema nos explica la existencia (o no) de máximos o mínimos locales en un punto dado.

> [!Important] **Importante**
> 
> Cuando $\Delta(x_0, y_0) < 0$ no se garantiza la existencia de máximos o mínimos.
### Multiplicadores de Lagrange

$$f(x,y,z,\lambda) = f(x,y,z) - \lambda g(x,y,z)$$
Donde:
* $f$ es la función objetivo.
* $\lambda$ es un parámetro.
* $g$ es la función de restricción.
